{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                                             # dense matrices\n",
    "import pandas as pd \n",
    "from scipy.sparse import csr_matrix                            # sparse matrices\n",
    "from sklearn.metrics.pairwise import pairwise_distances        # pairwise distances\n",
    "from copy import copy                                          # deep copies\n",
    "import matplotlib.pyplot as plt                                # plotting\n",
    "%matplotlib inline\n",
    "\n",
    "'''compute norm of a sparse vector\n",
    "   Thanks to: Jaiyam Sharma'''\n",
    "def norm(x):\n",
    "    sum_sq=x.dot(x.T)\n",
    "    norm=np.sqrt(sum_sq)\n",
    "    return(norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki = pd.read_csv('people_wiki.csv')\n",
    "wiki.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract TF-IDF vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the previous assignment, we extract the TF-IDF vector of each document.\n",
    "For your convenience, we extracted the TF-IDF vectors from the dataset. The vectors are packaged in a sparse matrix, where the i-th row gives the TF-IDF vectors for the i-th document. Each column corresponds to a unique word appearing in the dataset.To load in the TF-IDF vectors, run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sparse_csr(filename):\n",
    "    loader = np.load(filename)\n",
    "    data = loader['data']\n",
    "    indices = loader['indices']\n",
    "    indptr = loader['indptr']\n",
    "    shape = loader['shape']\n",
    "    return csr_matrix( (data, indices, indptr), shape)\n",
    "    \n",
    "corpus = load_sparse_csr('people_wiki_tf_idf.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word-to-index mapping is given by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "with open('people_wiki_map_index_to_word.json') as f:\n",
    "    map_index_to_word = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train an LSH model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSH performs an efficient neighbor search by randomly partitioning all reference data points into different bins. Today we will build a popular variant of LSH known as random binary projection, which approximates cosine distance. There are other variants we could use for other choices of distance metrics.\n",
    "\n",
    "The first step is to generate a collection of random vectors from the standard Gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_vectors(num_vector, dim):\n",
    "    return np.random.randn(dim, num_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now generate random vectors of the same dimensionality as our vocubulary size (547979). Each vector can be used to compute one bit in the bin encoding. We generate 16 vectors, leading to a 16-bit encoding of the bin index for each document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "random_vectors = generate_random_vectors(num_vector=16, dim=547979)\n",
    "print(random_vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we partition data points into bins. Instead of using explicit loops, we'd like to utilize matrix operations for greater efficiency. Let's walk through the construction step by step.\n",
    "\n",
    "We'd like to decide which bin document 0 should go. Since 16 random vectors were generated in the previous cell, we have 16 bits to represent the bin index. The first bit is given by the sign of the dot product between the first random vector and the document's TF-IDF vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = corpus[0, :] # vector of tf-idf values for document 0\n",
    "print(doc.dot(random_vectors[:, 0]) >= 0) # True if positive sign; False if negative sign"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, the second bit is computed as the sign of the dot product between the second random vector and the document vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc.dot(random_vectors[:, 1]) >= 0) # True if positive sign; False if negative sign"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute all of the bin index bits at once as follows. Note the absence of the explicit for loop over the 16 vectors. Matrix operations let us batch dot-product computation in a highly efficent manner, unlike the for loop construction. Given the relative inefficiency of loops in Python, the advantage of matrix operations is even greater. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc.dot(random_vectors) >= 0) # should return an array of 16 True/False bits\n",
    "print(np.array(doc.dot(random_vectors) >= 0, dtype=int)) # display index bits in 0/1's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All documents that obtain exactly this vector will be assigned to the same bin. We'd like to repeat the identical operation on all documents in the Wikipedia dataset and compute the corresponding bin indices. Again, we use matrix operations so that no explicit loop is needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus[0:2].dot(random_vectors) >= 0) # compute bit indices of first two documents\n",
    "print(corpus.dot(random_vectors) >= 0) # compute bit indices of ALL documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the rules of binary number representation, we just need to compute the dot product between the document vector and the vector consisting of powers of 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = corpus[0, :]  # first document\n",
    "index_bits = (doc.dot(random_vectors) >= 0)\n",
    "powers_of_two = (1 << np.arange(15, -1, -1))\n",
    "print(index_bits)\n",
    "print(powers_of_two)           # [32768, 16384, 8192, 4096, 2048, 1024, 512, 256, 128, 64, 32, 16, 8, 4, 2, 1]\n",
    "print(index_bits.dot(powers_of_two))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_bits = corpus.dot(random_vectors) >= 0\n",
    "print(index_bits)\n",
    "print(index_bits.dot(powers_of_two))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This array gives us the integer index of the bins for all documents.\n",
    "Now we are ready to complete the following function. Given the integer bin indices for the documents, you should compile a list of document IDs that belong to each bin. Since a list is to be maintained for each unique bin index, a dictionary of lists is used.\n",
    "\n",
    "1. Compute the integer bin indices. This step is already completed. \n",
    "2. For each document in the dataset, do the following: \n",
    "    - Get the integer bin index for the document. \n",
    "    - Fetch the list of document ids associated with the bin; if no list yet exists for this bin, assign the bin \n",
    "      an empty list. \n",
    "    - Add the document id to the end of the list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lsh(data, num_vector=16, seed=None):\n",
    "    \n",
    "    # generate 16 vectors, leading to a 16-bit encoding of the bin index for each document.\n",
    "    dim = data.shape[1]\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    random_vectors = generate_random_vectors(num_vector, dim)\n",
    "    \n",
    "    # vector consisting of powers of 2\n",
    "    powers_of_two = 1 << np.arange(num_vector-1, -1, -1)\n",
    "     \n",
    "    # Partition data points into bins\n",
    "    bin_index_bits = (data.dot(random_vectors) >= 0)\n",
    "  \n",
    "    # Encode bin index bits into integers\n",
    "    bin_indices = bin_index_bits.dot(powers_of_two)\n",
    "    \n",
    "    # Update `table` so that `table[i]` is the list of document ids with bin index equal to i.\n",
    "    table = {}\n",
    "    for data_index, bin_index in enumerate(bin_indices):\n",
    "        if bin_index not in table:\n",
    "            # If no list yet exists for this bin, assign the bin an empty list.\n",
    "            table[bin_index] = [] # YOUR CODE HERE\n",
    "        # Fetch the list of document ids associated with the bin and add the document id to the end.\n",
    "        table[bin_index].append(data_index)\n",
    "    \n",
    "    model = {'data': data,\n",
    "             'bin_index_bits': bin_index_bits,\n",
    "             'bin_indices': bin_indices,\n",
    "             'table': table,\n",
    "             'random_vectors': random_vectors,\n",
    "             'num_vector': num_vector}\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_lsh(corpus, num_vector=16, seed=143)\n",
    "table = model['table']\n",
    "if   0 in table and table[0]   == [39583] and \\\n",
    "   143 in table and table[143] == [19693, 28277, 29776, 30399]:\n",
    "    print('Passed!')\n",
    "else:\n",
    "    print('Check your code.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at some documents and see which bins they fall into.  Which bin contains Barack Obama's article? Enter its integer index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_obama = wiki[wiki['name'] == 'Barack Obama'].index[0]\n",
    "print(index_obama)\n",
    "index_bits_obama = corpus[index_obama,:].dot(random_vectors) >= 0\n",
    "print(\"INDEX BIT: \", index_bits_obama)\n",
    "int_index_bits_obama = index_bits_obama.dot(powers_of_two)\n",
    "print(\"INT INTEGER BIT: \", int_index_bits_obama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model['bin_indices'][index_obama]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the bit representations of the bins containing Barack Obama and Joe Biden. In how many places do they agree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_biden = wiki[wiki['name'] == 'Joe Biden'].index[0]\n",
    "index_bits_biden = corpus[index_biden,:].dot(random_vectors) >= 0\n",
    "print(\"INDEX BIT: \", index_bits_biden)\n",
    "int_index_bits_biden = index_bits_biden.dot(powers_of_two)\n",
    "print(\"INT INTEGER BIT: \", int_index_bits_biden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model['bin_index_bits'][index_obama] == model['bin_index_bits'][index_biden])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the result with a former British diplomat, whose bin representation agrees with Obama's in only 8 out of 16 places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_wynn = wiki[wiki['name']=='Wynn Normington Hugh-Jones'].index[0]\n",
    "print(wiki[wiki['name']=='Wynn Normington Hugh-Jones'])\n",
    "\n",
    "print(np.array(model['bin_index_bits'][index_wynn], dtype=int)) # list of 0/1's\n",
    "print(model['bin_index_bits'][index_obama] == model['bin_index_bits'][index_wynn])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about the documents in the same bin as Barack Obama? Are they necessarily more similar to Obama than Biden? Let's look at which documents are in the same bin as the Barack Obama article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model['table'][model['bin_indices'][index_obama]])\n",
    "doc_ids = list(model['table'][model['bin_indices'][index_obama]])\n",
    "doc_ids.remove(index_obama)\n",
    "\n",
    "docs = wiki.loc[doc_ids]\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that Joe Biden is much closer to Barack Obama than any of the four documents, even though Biden's bin representation differs from Obama's by 2 bits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_distance(x, y):\n",
    "    xy = x.dot(y.T)\n",
    "    dist = xy/(norm(x)*norm(y))\n",
    "    return 1-dist[0,0]\n",
    "\n",
    "obama_tf_idf = corpus[index_obama,:]\n",
    "biden_tf_idf = corpus[index_biden,:]\n",
    "\n",
    "print('================= Cosine distance from Barack Obama')\n",
    "print('Barack Obama - {0:24s}: {1:f}'.format('Joe Biden',\n",
    "                                             cosine_distance(obama_tf_idf, biden_tf_idf)))\n",
    "for doc_id in doc_ids:\n",
    "    doc_tf_idf = corpus[doc_id,:]\n",
    "    print('Barack Obama - {0:24s}: {1:f}'.format(wiki.loc[doc_id]['name'],\n",
    "                                                 cosine_distance(obama_tf_idf, doc_tf_idf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moral of the story. Similar data points will in general tend to fall into nearby bins, but that's all we can say about LSH. In a high-dimensional space such as text features, we often get unlucky with our selection of only a few random vectors such that dissimilar data points go into the same bin while similar data points fall into different bins. Given a query document, we must consider all documents in the nearby bins and sort them according to their actual distances from the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query the LSH model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first implement the logic for searching nearby neighbors, which goes like this:\n",
    "1. Let L be the bit representation of the bin that contains the query documents.\n",
    "2. Consider all documents in bin L.\n",
    "3. Consider documents in the bins whose bit representation differs from L by 1 bit.\n",
    "4. Consider documents in the bins whose bit representation differs from L by 2 bits.\n",
    "\n",
    "To obtain candidate bins that differ from the query bin by some number of bits, we use itertools.combinations, which produces all possible subsets of a given list. See this documentation for details.\n",
    "1. Decide on the search radius r. This will determine the number of different bits between the two vectors.\n",
    "2. For each subset (n_1, n_2, ..., n_r) of the list [0, 1, 2, ..., num_vector-1], do the following:\n",
    "   * Flip the bits (n_1, n_2, ..., n_r) of the query bin to produce a new bit vector.\n",
    "   * Fetch the list of documents belonging to the bin indexed by the new bit vector.\n",
    "   * Add those documents to the candidate set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_nearby_bins(query_bin_bits, table, search_radius=2, initial_candidates=set()):\n",
    "    \"\"\"\n",
    "    For a given query vector and trained LSH model, return all candidate neighbors for\n",
    "    the query among all bins within the given search radius.\n",
    "    \n",
    "    Example usage\n",
    "    -------------\n",
    "    >>> model = train_lsh(corpus, num_vector=16, seed=143)\n",
    "    >>> q = model['bin_index_bits'][0]  # vector for the first document\n",
    "  \n",
    "    >>> candidates = search_nearby_bins(q, model['table'])\n",
    "    \"\"\"\n",
    "    num_vector = len(query_bin_bits)\n",
    "    powers_of_two = 1 << np.arange(num_vector-1, -1, -1)\n",
    "    \n",
    "    # Allow the user to provide an initial set of candidates.\n",
    "    candidate_set = copy(initial_candidates)\n",
    "    \n",
    "    for different_bits in combinations(range(num_vector), search_radius):       \n",
    "        # Flip the bits (n_1,n_2,...,n_r) of the query bin to produce a new bit vector.\n",
    "        ## Hint: you can iterate over a tuple like a list\n",
    "        alternate_bits = copy(query_bin_bits)\n",
    "        for i in different_bits:\n",
    "            alternate_bits[i] = (alternate_bits[i] + 1) % 2 # YOUR CODE HERE \n",
    "        \n",
    "        # Convert the new bit vector to an integer index\n",
    "        nearby_bin = alternate_bits.dot(powers_of_two)\n",
    "        \n",
    "        # Fetch the list of documents belonging to the bin indexed by the new bit vector.\n",
    "        # Then add those documents to candidate_set\n",
    "        # Make sure that the bin exists in the table!\n",
    "        # Hint: update() method for sets lets you add an entire list to the set\n",
    "        if nearby_bin in table:\n",
    "            candidate_set.update((table[nearby_bin])) # YOUR CODE HERE: Update candidate_set with the documents in this bin.\n",
    "            \n",
    "    return candidate_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Checkpoint. Running the function with search_radius=0 should yield the list of documents belonging to the same bin as the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obama_bin_index = model['bin_index_bits'][35817] # bin index of Barack Obama\n",
    "print(obama_bin_index)\n",
    "candidate_set = search_nearby_bins(obama_bin_index, model['table'], search_radius=0)\n",
    "if candidate_set == set([35817, 21426, 53937, 39426, 50261]):\n",
    "    print('Passed test')\n",
    "else:\n",
    "    print('Check your code')\n",
    "print('List of documents in the same bin as Obama: 35817, 21426, 53937, 39426, 50261')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Checkpoint. Running the function with search_radius=1 adds more documents to the fore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_set = search_nearby_bins(obama_bin_index, model['table'], search_radius=1, initial_candidates=candidate_set)\n",
    "if candidate_set == set([39426, 38155, 38412, 28444, 9757, 41631, 39207, 59050, 47773, 53937, 21426, 34547,\n",
    "                         23229, 55615, 39877, 27404, 33996, 21715, 50261, 21975, 33243, 58723, 35817, 45676,\n",
    "                         19699, 2804, 20347]):\n",
    "    print('Passed test')\n",
    "else:\n",
    "    print('Check your code')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a function that can return all the candidates from neighboring bins. Next we write a function to collect all candidates and compute their true distance to the query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(vec, model, k, max_search_radius):\n",
    "  \n",
    "    data = model['data']\n",
    "    table = model['table']\n",
    "    random_vectors = model['random_vectors']\n",
    "    num_vector = random_vectors.shape[1]\n",
    "    \n",
    "    # Compute bin index for the query vector, in bit representation.\n",
    "    bin_index_bits = (vec.dot(random_vectors) >= 0).flatten()\n",
    "    \n",
    "    # Search nearby bins and collect candidates\n",
    "    candidate_set = set()\n",
    "    for search_radius in range(max_search_radius+1):\n",
    "        candidate_set = search_nearby_bins(bin_index_bits, table, search_radius, initial_candidates=candidate_set)\n",
    "    \n",
    "    # Sort candidates by their true distances from the query\n",
    "    nearest_neighbors = pd.DataFrame(index = candidate_set)\n",
    "    candidates = data[np.array(list(candidate_set)),:]\n",
    "    nearest_neighbors['distance'] = pairwise_distances(candidates, vec, metric='cosine').flatten()\n",
    "    \n",
    "    return nearest_neighbors.sort_values('distance', ascending=True)[:k], len(candidate_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out with Obama: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query(corpus[index_obama,:], model, k=10, max_search_radius=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To identify the documents, it's helpful to join this table with the Wikipedia table: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result, num_candidates_considered = query(corpus[index_obama,:], model, k=10, max_search_radius=3)\n",
    "print(result.join(wiki).sort_values('distance')[['distance', 'name']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with your LSH implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following sections we have implemented a few experiments so that you can gain intuition for how your LSH implementation behaves in different situations. This will help you understand the effect of searching nearby bins and the performance of LSH versus computing nearest neighbors using a brute force search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effect of nearby bin search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does nearby bin search affect the outcome of LSH? There are three variables that are affected by the search radius:\n",
    "    \n",
    "    - Number of candidate documents considered \n",
    "    - Query time \n",
    "    - Distance of approximate neighbors from the query\n",
    "\n",
    "Let us run LSH multiple times, each with different radii for nearby bin search. We will measure the three variables as discussed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "num_candidates_history = []\n",
    "query_time_history = []\n",
    "max_distance_from_query_history = []\n",
    "min_distance_from_query_history = []\n",
    "average_distance_from_query_history = []\n",
    "\n",
    "for max_search_radius in range(17):\n",
    "    start=time.time()\n",
    "    # Perform LSH query using Barack Obama, with max_search_radius\n",
    "    result, num_candidates = query(corpus[35817,:], model, k=10,\n",
    "                                   max_search_radius=max_search_radius)\n",
    "    end=time.time()\n",
    "    query_time = end-start  # Measure time\n",
    "    \n",
    "    print('Radius:', max_search_radius)\n",
    "    # Display 10 nearest neighbors, along with document ID and name\n",
    "    #print(result.join(wiki).sort_values(['distance']))\n",
    "    \n",
    "    # Collect statistics on 10 nearest neighbors\n",
    "    average_distance_from_query = result['distance'][1:].mean()\n",
    "    max_distance_from_query = result['distance'][1:].max()\n",
    "    min_distance_from_query = result['distance'][1:].min()\n",
    "    \n",
    "    num_candidates_history.append(num_candidates)\n",
    "    query_time_history.append(query_time)\n",
    "    average_distance_from_query_history.append(average_distance_from_query)\n",
    "    max_distance_from_query_history.append(max_distance_from_query)\n",
    "    min_distance_from_query_history.append(min_distance_from_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,4.5))\n",
    "plt.plot(num_candidates_history, linewidth=4)\n",
    "plt.xlabel('Search radius')\n",
    "plt.ylabel('# of documents searched')\n",
    "plt.rcParams.update({'font.size':16})\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.figure(figsize=(7,4.5))\n",
    "plt.plot(query_time_history, linewidth=4)\n",
    "plt.xlabel('Search radius')\n",
    "plt.ylabel('Query time (seconds)')\n",
    "plt.rcParams.update({'font.size':16})\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.figure(figsize=(7,4.5))\n",
    "plt.plot(average_distance_from_query_history, linewidth=4, label='Average of 10 neighbors')\n",
    "plt.plot(max_distance_from_query_history, linewidth=4, label='Farthest of 10 neighbors')\n",
    "plt.plot(min_distance_from_query_history, linewidth=4, label='Closest of 10 neighbors')\n",
    "plt.xlabel('Search radius')\n",
    "plt.ylabel('Cosine distance of neighbors')\n",
    "plt.legend(loc='best', prop={'size':15})\n",
    "plt.rcParams.update({'font.size':16})\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some observations:\n",
    "    \n",
    "    - As we increase the search radius, we find more neighbors that are a smaller distance away. \n",
    "    - With increased search radius comes a greater number documents that have to be searched. Query time is higher\n",
    "      as a consequence. \n",
    "    - With sufficiently high search radius, the results of LSH begin to resemble the results of brute-force search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_distance_from_query_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quality metrics for neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above analysis is limited by the fact that it was run with a single query, namely Barack Obama. We should repeat the analysis for the entirety of data. Iterating over all documents would take a long time, so let us randomly choose 10 documents for our analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each document, we first compute the true 25 nearest neighbors, and then run LSH multiple times. We look at two metrics:\n",
    "    \n",
    "    - Precision@10: How many of the 10 neighbors given by LSH are among the true 25 nearest neighbors? \n",
    "    - Average cosine distance of the neighbors from the query \n",
    "Then we run LSH multiple times with different search radii. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brute_force_query(vec, data, k):\n",
    "    num_data_points = data.shape[0]\n",
    "    \n",
    "    # Compute distances for ALL data points in training set\n",
    "    nearest_neighbors = pd.DataFrame( index = range(num_data_points))\n",
    "    nearest_neighbors['distance'] = pairwise_distances(data, vec, metric='cosine').flatten()\n",
    "    \n",
    "    return nearest_neighbors.sort_values(['distance'],ascending=True)[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will run LSH with multiple search radii and compute the quality metrics for each run. Allow a few minutes to complete. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_radius = 17\n",
    "precision = {i:[] for i in range(max_radius)}\n",
    "average_distance  = {i:[] for i in range(max_radius)}\n",
    "query_time  = {i:[] for i in range(max_radius)}\n",
    "\n",
    "np.random.seed(0)\n",
    "num_queries = 10\n",
    "for i, ix in enumerate(np.random.choice(corpus.shape[0], num_queries, replace=False)):\n",
    "    print('%s / %s' % (i, num_queries))\n",
    "    ground_truth =set(brute_force_query(corpus[ix,:], corpus, k=25).index.values)\n",
    "    # Get the set of 25 true nearest neighbors\n",
    "    \n",
    "    for r in range(1,max_radius):\n",
    "        start = time.time()\n",
    "        result, num_candidates = query(corpus[ix,:], model, k=10, max_search_radius=r)\n",
    "        end = time.time()\n",
    "\n",
    "        query_time[r].append(end-start)\n",
    "        # precision = (# of neighbors both in result and ground_truth)/10.0\n",
    "        precision[r].append(len(set(result.index.values) & ground_truth)/10.0)\n",
    "        average_distance[r].append(result['distance'][1:].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,4.5))\n",
    "plt.plot(range(1,17), [np.mean(average_distance[i]) for i in range(1,17)], linewidth=4, label='Average over 10 neighbors')\n",
    "plt.xlabel('Search radius')\n",
    "plt.ylabel('Cosine distance')\n",
    "plt.legend(loc='best', prop={'size':15})\n",
    "plt.rcParams.update({'font.size':16})\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.figure(figsize=(7,4.5))\n",
    "plt.plot(range(1,17), [np.mean(precision[i]) for i in range(1,17)], linewidth=4, label='Precison@10')\n",
    "plt.xlabel('Search radius')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend(loc='best', prop={'size':15})\n",
    "plt.rcParams.update({'font.size':16})\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.figure(figsize=(7,4.5))\n",
    "plt.plot(range(1,17), [np.mean(query_time[i]) for i in range(1,17)], linewidth=4, label='Query time')\n",
    "plt.xlabel('Search radius')\n",
    "plt.ylabel('Query time (seconds)')\n",
    "plt.legend(loc='best', prop={'size':15})\n",
    "plt.rcParams.update({'font.size':16})\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effect of number of random vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now turn our focus to the remaining parameter: the number of random vectors. We run LSH with different number of random vectors, ranging from 5 to 20. We fix the search radius to 3.\n",
    "Allow a few minutes for the following cell to complete. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = {i:[] for i in range(5,20)}\n",
    "average_distance  = {i:[] for i in range(5,20)}\n",
    "query_time = {i:[] for i in range(5,20)}\n",
    "num_candidates_history = {i:[] for i in range(5,20)}\n",
    "ground_truth = {}\n",
    "\n",
    "np.random.seed(0)\n",
    "num_queries = 10\n",
    "docs = np.random.choice(corpus.shape[0], num_queries, replace=False)\n",
    "\n",
    "for i, ix in enumerate(docs):\n",
    "    ground_truth[ix] = set(brute_force_query(corpus[ix,:], corpus, k=25).index.values)\n",
    "    # Get the set of 25 true nearest neighbors\n",
    "\n",
    "for num_vector in range(5,20):\n",
    "    print('num_vector = %s' % (num_vector))\n",
    "    model = train_lsh(corpus, num_vector, seed=143)\n",
    "    \n",
    "    for i, ix in enumerate(docs):\n",
    "        start = time.time()\n",
    "        result, num_candidates = query(corpus[ix,:], model, k=10, max_search_radius=3)\n",
    "        end = time.time()\n",
    "        \n",
    "        query_time[num_vector].append(end-start)\n",
    "        precision[num_vector].append(len(set(result.index.values) & ground_truth[ix])/10.0)\n",
    "        average_distance[num_vector].append(result['distance'][1:].mean())\n",
    "        num_candidates_history[num_vector].append(num_candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,4.5))\n",
    "plt.plot(range(5,20), [np.mean(average_distance[i]) for i in range(5,20)], linewidth=4, label='Average over 10 neighbors')\n",
    "plt.xlabel('# of random vectors')\n",
    "plt.ylabel('Cosine distance')\n",
    "plt.legend(loc='best', prop={'size':15})\n",
    "plt.rcParams.update({'font.size':16})\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.figure(figsize=(7,4.5))\n",
    "plt.plot(range(5,20), [np.mean(precision[i]) for i in range(5,20)], linewidth=4, label='Precison@10')\n",
    "plt.xlabel('# of random vectors')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend(loc='best', prop={'size':15})\n",
    "plt.rcParams.update({'font.size':16})\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.figure(figsize=(7,4.5))\n",
    "plt.plot(range(5,20), [np.mean(query_time[i]) for i in range(5,20)], linewidth=4, label='Query time (seconds)')\n",
    "plt.xlabel('# of random vectors')\n",
    "plt.ylabel('Query time (seconds)')\n",
    "plt.legend(loc='best', prop={'size':15})\n",
    "plt.rcParams.update({'font.size':16})\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.figure(figsize=(7,4.5))\n",
    "plt.plot(range(5,20), [np.mean(num_candidates_history[i]) for i in range(5,20)], linewidth=4,\n",
    "         label='# of documents searched')\n",
    "plt.xlabel('# of random vectors')\n",
    "plt.ylabel('# of documents searched')\n",
    "plt.legend(loc='best', prop={'size':15})\n",
    "plt.rcParams.update({'font.size':16})\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a similar trade-off between quality and performance: as the number of random vectors increases, the query time goes down as each bin contains fewer documents on average, but on average the neighbors are likewise placed farther from the query. On the other hand, when using a small enough number of random vectors, LSH becomes very similar brute-force search: Many documents appear in a single bin, so searching the query bin alone covers a lot of the corpus; then, including neighboring bins might result in searching all documents, just as in the brute-force approach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "(intel-2020) Python 3",
   "language": "python",
   "name": "conda-env-intel-2020-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
